{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d153da36",
   "metadata": {},
   "source": [
    "### Theory of GAN (Generative Adversarial Networks):\n",
    "\n",
    "GANs consist of two neural networks: a generator and a discriminator, which are trained simultaneously through a competitive process.\n",
    "\n",
    "1. **Generator (G)**: This network takes random noise as input and generates fake data samples (in our case, fashion images).\n",
    "\n",
    "2. **Discriminator (D)**: This network aims to distinguish between real (from the training set) and fake (generated by the generator) samples.\n",
    "\n",
    "During training, the generator tries to produce samples that are indistinguishable from real data, while the discriminator tries to correctly classify real and fake samples. As training progresses, both networks improve until the generator produces high-quality samples.\n",
    "\n",
    "### Building Generator and Discriminator Models for Fashion MNIST:\n",
    "\n",
    "#### 1. Generator Model:\n",
    "- Input: Random noise vector (latent space).\n",
    "- Output: Synthetic fashion images.\n",
    "- Architecture: Typically consists of transpose convolution layers followed by activation functions like ReLU and a sigmoid function at the output to scale pixel values between 0 and 1.\n",
    "\n",
    "#### 2. Discriminator Model:\n",
    "- Input: Fashion images (real or synthetic).\n",
    "- Output: Probability of the input being real.\n",
    "- Architecture: Convolutional layers followed by activation functions like LeakyReLU, and finally a sigmoid function to produce a probability score.\n",
    "\n",
    "Let's code the generator and discriminator models:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        layers.Dense(7 * 7 * 256, input_dim=latent_dim),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=input_shape),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the latent space dimension\n",
    "latent_dim = 100\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "input_shape = (28, 28, 1)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summaries\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "```\n",
    "\n",
    "These models define the architecture for the generator and discriminator. Now, you can train them using Fashion MNIST data. If you need help with training or further explanation, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d856306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y tensorflow\n",
    "# !python -m pip install -U tensorflow-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd31b4",
   "metadata": {},
   "source": [
    "# 1. Importing Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf205ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting GPU memory growth\n",
    "# gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3721b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077663c3",
   "metadata": {},
   "source": [
    "## Import Dataset and Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec406d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39d097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9577a757",
   "metadata": {},
   "source": [
    "## class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8584642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.expand_dims(x_train/255, axis=-1), y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.expand_dims(x_test/255, axis=-1), y_test))\n",
    "\n",
    "\n",
    "# Optionally shuffle, batch, Cache and prefetch the dataset\n",
    "batch_size = 128\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train))\n",
    "\n",
    "train_ds = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiterator = train_dataset.as_numpy_iterator()\n",
    "plt.imshow(dataiterator.next()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiterator.next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe72aa4",
   "metadata": {},
   "source": [
    "## Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d64bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc60135",
   "metadata": {},
   "source": [
    "# 2. Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42832c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))  # Create a figure with 1 row and 4 columns\n",
    "\n",
    "# Generate 4 random indices\n",
    "random_indices = random.sample(range(len(x_train)), 4)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax[i].imshow(x_train[idx])\n",
    "    ax[i].axis('off')  # Turn off axis\n",
    "    ax[i].set_title(f'{class_labels[y_train[idx]]}')  # Set title for each image\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16a087",
   "metadata": {},
   "source": [
    "# 3. Build Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd78933",
   "metadata": {},
   "source": [
    "## 3.1 Import Modelling Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86810086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the Sequential api for the generator and discriminator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D , Dense , Flatten , UpSampling2D , ReLU , LeakyReLU , Dropout , BatchNormalization , Reshape, Conv2DTranspose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a839c",
   "metadata": {},
   "source": [
    "## 3.2 Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e841031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset TensorFlow graph\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        Dense(7 * 7 * 256, input_dim=latent_dim),\n",
    "        Reshape((7, 7, 256)),\n",
    "        Conv2D(128, (5, 5), strides=(1, 1), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2D(1, (5, 5), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df223b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator((128,28,28,1))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d9c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
